{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137f2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import re \n",
    "import string \n",
    "import tensorflow as tf \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0da19e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/DELL/Downloads/contradictory-my-dear-watson'\n",
    "train_data = pd.read_csv(path + '/train.csv')\n",
    "test_data = pd.read_csv(path + '/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fe9554a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>lang_abv</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5130fd2cb5</td>\n",
       "      <td>and these comments were considered in formulat...</td>\n",
       "      <td>The rules developed in the interim were put to...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b72532a0b</td>\n",
       "      <td>These are issues that we wrestle with in pract...</td>\n",
       "      <td>Practice groups are not permitted to work on t...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3931fbe82a</td>\n",
       "      <td>Des petites choses comme celles-là font une di...</td>\n",
       "      <td>J'essayais d'accomplir quelque chose.</td>\n",
       "      <td>fr</td>\n",
       "      <td>French</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5622f0c60b</td>\n",
       "      <td>you know they can't really defend themselves l...</td>\n",
       "      <td>They can't defend themselves because of their ...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86aaa48b45</td>\n",
       "      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n",
       "      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n",
       "      <td>th</td>\n",
       "      <td>Thai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            premise  \\\n",
       "0  5130fd2cb5  and these comments were considered in formulat...   \n",
       "1  5b72532a0b  These are issues that we wrestle with in pract...   \n",
       "2  3931fbe82a  Des petites choses comme celles-là font une di...   \n",
       "3  5622f0c60b  you know they can't really defend themselves l...   \n",
       "4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
       "\n",
       "                                          hypothesis lang_abv language  label  \n",
       "0  The rules developed in the interim were put to...       en  English      0  \n",
       "1  Practice groups are not permitted to work on t...       en  English      2  \n",
       "2              J'essayais d'accomplir quelque chose.       fr   French      0  \n",
       "3  They can't defend themselves because of their ...       en  English      0  \n",
       "4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb637a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "193ab945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(documents, embedding_dim):\n",
    "    \"\"\"\n",
    "    train word2vector over traning documents\n",
    "    Args:\n",
    "        documents (list): list of document\n",
    "        embedding_dim (int): outpu wordvector size\n",
    "    Returns:\n",
    "        word_vectors(dict): dict containing words and their respective vectors\n",
    "    \"\"\"\n",
    "    model = Word2Vec(documents, min_count=1, vector_size=embedding_dim)\n",
    "    word_vectors = model.wv\n",
    "    del model\n",
    "    return word_vectors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21b4a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(tokenizer, word_vectors, embedding_dim):\n",
    "    \"\"\"\n",
    "    Create embedding matrix containing word indexes and respective vectors from word vectors\n",
    "    Args:\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object containing word indexes\n",
    "        word_vectors (dict): dict containing word and their respective vectors\n",
    "        embedding_dim (int): dimention of word vector\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    nb_words = len(tokenizer.word_index) + 1\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "    print(\"Embedding matrix shape: %s\" % str(embedding_matrix.shape))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            print(\"vector not found for word - %s\" % word)\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "978fd012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embed_meta_data(documents, embedding_dim):\n",
    "    \"\"\"\n",
    "    Load tokenizer object for given vocabs list\n",
    "    Args:\n",
    "        documents (list): list of document\n",
    "        embedding_dim (int): embedding dimension\n",
    "    Returns:\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
    "        embedding_matrix (dict): dict with word_index and vector mapping\n",
    "    \"\"\"\n",
    "    documents = [x.lower().split() for x in documents]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(documents)\n",
    "    word_vector = train_word2vec(documents, embedding_dim)\n",
    "    embedding_matrix = create_embedding_matrix(tokenizer, word_vector, embedding_dim)\n",
    "    del word_vector\n",
    "    gc.collect()\n",
    "    return tokenizer, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d63980bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "premises = list(train_data['premise'])\n",
    "hypothesises = list(train_data['hypothesis'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8550d64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (64542, 300)\n",
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "tokenizer , embedding_matrix = word_embed_meta_data(premises  + hypothesises  , EMBEDDING_DIM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "daed8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_val_data(data , tokenizer ,max_sequence_length)  :\n",
    "    labels = data.label \n",
    "    labels = np.array(labels)\n",
    "    premises = data['premise']\n",
    "    hypothesises = data['hypothesis']\n",
    "    train_premises = tokenizer.texts_to_sequences(premises)\n",
    "    train_hypothesises = tokenizer.texts_to_sequences(hypothesises)\n",
    "    train_premises = pad_sequences(train_premises , maxlen = MAX_SEQUENCE_LENGTH)\n",
    "    train_hypothesises = pad_sequences(train_hypothesises , maxlen = MAX_SEQUENCE_LENGTH)\n",
    "    X_P_train , X_P_val , train_labels , val_labels = train_test_split(train_premises , labels  ,test_size= .15 ,random_state =123 )\n",
    "    X_H_train , X_H_val , _ , _ = train_test_split(train_hypothesises , labels  ,test_size = .15 ,random_state =123)\n",
    "    return X_P_train , X_P_val ,X_H_train , X_H_val ,train_labels , val_labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "91e4a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(data,tokenizer ,max_sequence_length):  \n",
    "    premises = data['premise']\n",
    "    hypothesises = data['hypothesis']\n",
    "    test_premises = tokenizer.texts_to_sequences(premises)\n",
    "    test_hypothesises = tokenizer.texts_to_sequences(hypothesises)\n",
    "    X_P_test = pad_sequences(test_premises , maxlen = MAX_SEQUENCE_LENGTH)\n",
    "    X_H_test = pad_sequences(test_hypothesises , maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    return X_P_test , X_H_test ,  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6611528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, LSTM, Dropout, Bidirectional ,BatchNormalization , concatenate , Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "76b7fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(tokenizer.word_index) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "816c393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class siamese_model(tf.keras.Model) : \n",
    "    def __init__(self , embedding_dim, max_sequence_length,nb_words) : \n",
    "        super(siamese_model , self ).__init__() \n",
    "        # this the unified part of the network \n",
    "        self.embed = Embedding(nb_words , embedding_dim , weights=[embedding_matrix],\n",
    "                                    input_length=max_sequence_length, trainable=False)\n",
    "        self.lstm_1 = Bidirectional(LSTM(64 , return_sequences = True))\n",
    "        self.lstm_dropout = Dropout(.5)\n",
    "        self.lstm_2 = Bidirectional(LSTM(64))\n",
    "        \n",
    "\n",
    "        # rest of the model \n",
    "        self.dense_1 = Dense(256 , activation ='relu')\n",
    "        self.batch_norm = BatchNormalization()\n",
    "        self.drop_1 = Dropout(.5) \n",
    "        self.dense_2 = Dense(128 , activation ='relu')\n",
    "        self.dense_3 = Dense(64 , activation ='relu')\n",
    "        self.drop_2 = Dropout(.5)\n",
    "        self.out = Dense(3 , activation = 'softmax')\n",
    "\n",
    "    def call(self , inputs) : \n",
    "        premises , hypothesises = inputs[0] , inputs[1] \n",
    "        # premises part \n",
    "        X_P = self.embed(premises)\n",
    "        X_P = self.lstm_1(X_P)\n",
    "        X_P = self.lstm_dropout(X_P)\n",
    "        X_P = self.lstm_2(X_P)\n",
    "        \n",
    "        # hypothesises part \n",
    "        X_H = self.embed(hypothesises)\n",
    "        X_H = self.lstm_1(X_H)\n",
    "        X_H = self.lstm_dropout(X_H)\n",
    "        X_H = self.lstm_2(X_H)\n",
    "        \n",
    "        \n",
    "        # concat \n",
    "        X_C = concatenate([X_P , X_H  ])\n",
    "        X_C = self.dense_1(X_C)\n",
    "        X_C = self.batch_norm(X_C)\n",
    "        X_C = self.drop_1(X_C)\n",
    "        X_C = self.dense_2(X_C)\n",
    "        X_C = self.dense_3(X_C)\n",
    "        X_C = self.drop_2(X_C)\n",
    "        return  self.out(X_C)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2676c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_P_train , X_P_val ,X_H_train , X_H_val ,train_labels , val_labels  =process_train_val_data(train_data,tokenizer ,MAX_SEQUENCE_LENGTH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "34d5b847",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_P_test , X_H_test  = process_test_data(test_data ,tokenizer ,MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5254ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = siamese_model(EMBEDDING_DIM ,MAX_SEQUENCE_LENGTH , nb_words ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7e3d0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch : 1e-8 * 10**(epoch / 2)) \n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "65be8bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7eb9d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer =optimizer , loss ='sparse_categorical_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "19785fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "161/161 [==============================] - 78s 382ms/step - loss: 1.4741 - accuracy: 0.3261 - val_loss: 1.1018 - val_accuracy: 0.3130 - lr: 1.0000e-08\n",
      "Epoch 2/100\n",
      "161/161 [==============================] - 59s 366ms/step - loss: 1.4759 - accuracy: 0.3328 - val_loss: 1.1055 - val_accuracy: 0.3190 - lr: 3.1623e-08\n",
      "Epoch 3/100\n",
      "161/161 [==============================] - 55s 342ms/step - loss: 1.4520 - accuracy: 0.3408 - val_loss: 1.1194 - val_accuracy: 0.3207 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "161/161 [==============================] - 54s 333ms/step - loss: 1.4638 - accuracy: 0.3367 - val_loss: 1.1405 - val_accuracy: 0.3256 - lr: 3.1623e-07\n",
      "Epoch 5/100\n",
      "161/161 [==============================] - 53s 327ms/step - loss: 1.4713 - accuracy: 0.3285 - val_loss: 1.1491 - val_accuracy: 0.3300 - lr: 1.0000e-06\n",
      "Epoch 6/100\n",
      "161/161 [==============================] - 52s 326ms/step - loss: 1.4624 - accuracy: 0.3196 - val_loss: 1.1384 - val_accuracy: 0.3339 - lr: 3.1623e-06\n"
     ]
    }
   ],
   "source": [
    "Hist = model.fit([X_P_train , X_H_train  ] , train_labels , validation_data =([X_P_val , X_H_val ] , val_labels), \n",
    "                epochs =100 , callbacks =[lr_schedule , early_stop] , batch_size =64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e1550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
